"""
CogVideoX1.5 è§†é¢‘é®æŒ¡æ·»åŠ å·¥å…· - ä¿®å¤ç‰ˆ
ä¿®å¤äº†ä¾èµ–åº“å…¼å®¹æ€§é—®é¢˜

ä½¿ç”¨æ–¹æ³•:
    python video_occlusion_fixed.py

ä½œè€…: AI Assistant
æ—¥æœŸ: 2025-10-06
"""

import os
import random
import torch
import cv2
from pathlib import Path
from diffusers import CogVideoXImageToVideoPipeline
from diffusers.utils import export_to_video
from PIL import Image
import warnings
import time
warnings.filterwarnings('ignore')

# ==================== é…ç½®åŒºåŸŸ ====================
CONFIG = {
    # è·¯å¾„é…ç½®
    'video_input_dir': "/root/autodl-tmp/sida/urban_videos/original",
    'model_dir': "/root/autodl-tmp/sida/occlusion_new/models",
    'output_dir': "/root/autodl-tmp/sida/urban_videos/with_occlusion",
    
    # æ¨¡å‹é…ç½®
    'model_id': "THUDM/CogVideoX1.5-5B-I2V",
    
    # ç”Ÿæˆå‚æ•°
    'num_inference_steps': 50,
    'num_frames': 81,
    'guidance_scale': 6.0,
    'fps': 16,
    'seed': 42,
    
    # å†…å­˜ä¼˜åŒ–
    'use_cpu_offload': False,
    'use_vae_slicing': True,
    'use_vae_tiling': True,
    'dtype': torch.bfloat16,
    
    # å¤„ç†é€‰é¡¹
    'random_occlusion': True,
    'max_size': 1360,
    'min_size': 768,
    'batch_size': 1,
}

# é®æŒ¡æç¤ºè¯
OCCLUSION_PROMPTS = [
    "A close-up car in the foreground partially blocking the view, with a vehicle silhouette passing by the camera, creating a natural street perspective with realistic automobile obstruction",
    "Tree branches and leaves in the foreground, natural foliage partially blocking the camera view, with a tree trunk visible in front, creating an outdoor perspective with vegetation obstruction",
    "A building corner in the foreground, architectural elements blocking part of the view, wall edge obscuring the perspective, urban structure creating a natural city obstruction",
    "A person walking directly in front of the camera, pedestrian silhouette in the foreground, human figure partially blocking the street view, realistic passerby obstruction",
    "Traffic sign pole in the foreground, street signage blocking part of the view, urban infrastructure obstruction with road signs clearly visible in front",
    "Various foreground objects partially blocking the view, natural urban elements obstructing the camera, street furniture creating realistic perspective with obstacles",
    "Fence or metal railing in the foreground, barrier partially blocking the street view, guardrail obstruction creating depth in the urban scene",
    "Bus stop shelter in the foreground, urban furniture blocking part of the view, street infrastructure with city elements clearly visible in front of camera"
]


class VideoOcclusionProcessor:
    """è§†é¢‘é®æŒ¡å¤„ç†å™¨ - ä¿®å¤ç‰ˆ"""
    
    def __init__(self, config):
        self.config = config
        self.pipe = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.processing_stats = {
            'total_videos': 0,
            'successful': 0,
            'failed': 0,
            'total_time': 0
        }
        
    def print_system_info(self):
        """æ‰“å°ç³»ç»Ÿä¿¡æ¯"""
        print("\n" + "="*80)
        print("ç³»ç»Ÿä¿¡æ¯".center(80))
        print("="*80)
        
        if torch.cuda.is_available():
            print(f"âœ“ GPU: {torch.cuda.get_device_name(0)}")
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"âœ“ æ˜¾å­˜: {total_memory:.2f} GB")
            print(f"âœ“ CUDAç‰ˆæœ¬: {torch.version.cuda}")
        else:
            print("âœ— è­¦å‘Š: æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPU (éå¸¸æ…¢)")
            
        print(f"âœ“ PyTorchç‰ˆæœ¬: {torch.__version__}")
        
        # æ‰“å°å…³é”®ä¾èµ–ç‰ˆæœ¬
        try:
            import transformers
            import diffusers
            import accelerate
            print(f"âœ“ Transformersç‰ˆæœ¬: {transformers.__version__}")
            print(f"âœ“ Diffusersç‰ˆæœ¬: {diffusers.__version__}")
            print(f"âœ“ Accelerateç‰ˆæœ¬: {accelerate.__version__}")
        except ImportError as e:
            print(f"âš ï¸  è­¦å‘Š: æ— æ³•è·å–æŸäº›ä¾èµ–ç‰ˆæœ¬ - {e}")
            
        print(f"âœ“ è®¾å¤‡: {self.device}")
        print("="*80 + "\n")
        
    def setup_model(self):
        """åŠ è½½æ¨¡å‹ - ä¿®å¤ç‰ˆ"""
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {self.config['model_id']}")
        print(f"æ¨¡å‹ä¿å­˜è·¯å¾„: {self.config['model_dir']}\n")
        
        os.makedirs(self.config['model_dir'], exist_ok=True)
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
        os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'
        
        start_time = time.time()
        
        try:
            print("ğŸ“¦ æ­£åœ¨åŠ è½½æ¨¡å‹åˆ°å†…å­˜...")
            
            # ä¿®å¤ï¼šä¸ä¼ é€’ä¸å…¼å®¹çš„å‚æ•°
            self.pipe = CogVideoXImageToVideoPipeline.from_pretrained(
                self.config['model_id'],
                torch_dtype=self.config['dtype'],
                cache_dir=self.config['model_dir'],
                # ç§»é™¤äº† resume_download å’Œ local_files_only å‚æ•°
            )
            
        except Exception as e:
            print(f"\nâŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("\nğŸ”§ å°è¯•ä½¿ç”¨å¤‡ç”¨åŠ è½½æ–¹å¼...")
            
            try:
                # å¤‡ç”¨æ–¹æ¡ˆï¼šåˆ†æ­¥åŠ è½½
                from transformers import T5EncoderModel, T5Tokenizer
                
                print("   1. åŠ è½½æ–‡æœ¬ç¼–ç å™¨...")
                text_encoder = T5EncoderModel.from_pretrained(
                    self.config['model_id'],
                    subfolder="text_encoder",
                    torch_dtype=self.config['dtype'],
                    cache_dir=self.config['model_dir'],
                )
                
                print("   2. åŠ è½½å®Œæ•´pipeline...")
                self.pipe = CogVideoXImageToVideoPipeline.from_pretrained(
                    self.config['model_id'],
                    text_encoder=text_encoder,
                    torch_dtype=self.config['dtype'],
                    cache_dir=self.config['model_dir'],
                )
                print("âœ“ å¤‡ç”¨åŠ è½½æ–¹å¼æˆåŠŸ!")
                
            except Exception as e2:
                print(f"\nâŒ å¤‡ç”¨åŠ è½½ä¹Ÿå¤±è´¥: {e2}")
                print("\nğŸ’¡ å»ºè®®:")
                print("1. æ›´æ–°ä¾èµ–åº“:")
                print("   pip install --upgrade transformers diffusers accelerate")
                print("2. æˆ–å®‰è£…æŒ‡å®šç‰ˆæœ¬:")
                print("   pip install transformers==4.46.0 diffusers==0.31.0")
                raise
        
        # åº”ç”¨ä¼˜åŒ–
        if self.config['use_cpu_offload']:
            print("âœ“ å¯ç”¨CPU offloadä¼˜åŒ–")
            self.pipe.enable_model_cpu_offload()
        else:
            print("âœ“ æ¨¡å‹åŠ è½½åˆ°GPU")
            self.pipe.to(self.device)
            
        if self.config['use_vae_slicing']:
            print("âœ“ å¯ç”¨VAEåˆ‡ç‰‡ä¼˜åŒ–")
            self.pipe.vae.enable_slicing()
            
        if self.config['use_vae_tiling']:
            print("âœ“ å¯ç”¨VAEå¹³é“ºä¼˜åŒ–")
            self.pipe.vae.enable_tiling()
        
        load_time = time.time() - start_time
        print(f"\nâœ“ æ¨¡å‹åŠ è½½å®Œæˆ! æ€»è€—æ—¶: {load_time:.2f}ç§’\n")
        
    def extract_first_frame(self, video_path):
        """æå–è§†é¢‘ç¬¬ä¸€å¸§"""
        cap = cv2.VideoCapture(video_path)
        ret, frame = cap.read()
        
        if not ret:
            cap.release()
            raise ValueError(f"æ— æ³•è¯»å–è§†é¢‘: {video_path}")
        
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        cap.release()
        
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        return Image.fromarray(frame_rgb), fps, total_frames
    
    def resize_image(self, image):
        """è°ƒæ•´å›¾åƒå°ºå¯¸"""
        w, h = image.size
        
        min_dim = min(w, h)
        max_dim = max(w, h)
        
        if min_dim < self.config['min_size']:
            scale = self.config['min_size'] / min_dim
            new_w, new_h = int(w * scale), int(h * scale)
        elif max_dim > self.config['max_size']:
            scale = self.config['max_size'] / max_dim
            new_w, new_h = int(w * scale), int(h * scale)
        else:
            new_w, new_h = w, h
        
        new_w = (new_w // 16) * 16
        new_h = (new_h // 16) * 16
        
        return image.resize((new_w, new_h), Image.Resampling.LANCZOS)
    
    def generate_video(self, video_path, output_path, occlusion_prompt):
        """ç”Ÿæˆå¸¦é®æŒ¡çš„è§†é¢‘"""
        print(f"\n{'='*80}")
        print(f"å¤„ç†è§†é¢‘: {os.path.basename(video_path)}")
        print(f"{'='*80}")
        
        start_time = time.time()
        
        try:
            first_frame, original_fps, total_frames = self.extract_first_frame(video_path)
            original_size = first_frame.size
            
            first_frame = self.resize_image(first_frame)
            print(f"ğŸ“ å›¾åƒå°ºå¯¸: {original_size} -> {first_frame.size}")
            print(f"ğŸ¬ åŸå§‹è§†é¢‘: {original_fps}fps, {total_frames}å¸§")
            
            full_prompt = (
                f"A high-quality urban street scene with realistic motion and natural lighting. "
                f"{occlusion_prompt}. "
                f"The scene maintains consistent dynamics and smooth motion. "
                f"Cinematic quality, detailed foreground and background, natural perspective."
            )
            
            print(f"ğŸ“ é®æŒ¡æ•ˆæœ: {occlusion_prompt[:70]}...")
            print(f"\nâš™ï¸  ç”Ÿæˆå‚æ•°:")
            print(f"   - æ¨ç†æ­¥æ•°: {self.config['num_inference_steps']}")
            print(f"   - ç”Ÿæˆå¸§æ•°: {self.config['num_frames']}")
            print(f"   - è¾“å‡ºå¸§ç‡: {self.config['fps']}fps")
            print(f"   - å¼•å¯¼ç³»æ•°: {self.config['guidance_scale']}")
            
            print(f"\nğŸ¨ æ­£åœ¨ç”Ÿæˆè§†é¢‘...")
            
            generator = torch.Generator(device=self.device)
            if self.config['seed'] is not None:
                generator.manual_seed(self.config['seed'])
            
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            video_frames = self.pipe(
                prompt=full_prompt,
                image=first_frame,
                num_videos_per_prompt=1,
                num_inference_steps=self.config['num_inference_steps'],
                num_frames=self.config['num_frames'],
                guidance_scale=self.config['guidance_scale'],
                generator=generator,
            ).frames[0]
            
            print(f"ğŸ’¾ ä¿å­˜è§†é¢‘...")
            export_to_video(video_frames, output_path, fps=self.config['fps'])
            
            generation_time = time.time() - start_time
            print(f"\nâœ“ å®Œæˆ! è€—æ—¶: {generation_time:.2f}ç§’")
            print(f"âœ“ ä¿å­˜è‡³: {output_path}\n")
            
            return True, generation_time
            
        except Exception as e:
            error_time = time.time() - start_time
            print(f"\nâœ— é”™è¯¯: {str(e)}")
            print(f"âœ— å¤±è´¥è€—æ—¶: {error_time:.2f}ç§’\n")
            return False, error_time
    
    def process_all_videos(self):
        """æ‰¹é‡å¤„ç†æ‰€æœ‰è§†é¢‘"""
        os.makedirs(self.config['output_dir'], exist_ok=True)
        
        video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.MP4', '.AVI', '.MOV']
        video_files = []
        for ext in video_extensions:
            video_files.extend(Path(self.config['video_input_dir']).glob(f'*{ext}'))
        
        if not video_files:
            print(f"âŒ é”™è¯¯: åœ¨ {self.config['video_input_dir']} ä¸­æœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶!")
            return
        
        self.processing_stats['total_videos'] = len(video_files)
        print(f"\nğŸ“ æ‰¾åˆ° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶\n")
        
        print("å¾…å¤„ç†è§†é¢‘:")
        for i, vf in enumerate(video_files, 1):
            print(f"  {i}. {vf.name}")
        print()
        
        for i, video_path in enumerate(video_files, 1):
            print(f"\n{'#'*80}")
            print(f"è¿›åº¦: [{i}/{len(video_files)}]")
            print(f"{'#'*80}")
            
            if self.config['random_occlusion']:
                occlusion_prompt = random.choice(OCCLUSION_PROMPTS)
            else:
                occlusion_prompt = OCCLUSION_PROMPTS[i % len(OCCLUSION_PROMPTS)]
            
            output_filename = f"{video_path.stem}_occluded.mp4"
            output_path = os.path.join(self.config['output_dir'], output_filename)
            
            success, duration = self.generate_video(
                str(video_path),
                output_path,
                occlusion_prompt
            )
            
            if success:
                self.processing_stats['successful'] += 1
            else:
                self.processing_stats['failed'] += 1
            
            self.processing_stats['total_time'] += duration
            
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        self.print_summary()
    
    def print_summary(self):
        """æ‰“å°å¤„ç†æ‘˜è¦"""
        print("\n" + "="*80)
        print("å¤„ç†å®Œæˆç»Ÿè®¡".center(80))
        print("="*80)
        print(f"æ€»è§†é¢‘æ•°: {self.processing_stats['total_videos']}")
        print(f"æˆåŠŸ: {self.processing_stats['successful']} âœ“")
        print(f"å¤±è´¥: {self.processing_stats['failed']} âœ—")
        print(f"æ€»è€—æ—¶: {self.processing_stats['total_time']:.2f}ç§’")
        
        if self.processing_stats['successful'] > 0:
            avg_time = self.processing_stats['total_time'] / self.processing_stats['successful']
            print(f"å¹³å‡æ¯ä¸ªè§†é¢‘: {avg_time:.2f}ç§’")
        
        print(f"\nè¾“å‡ºç›®å½•: {self.config['output_dir']}")
        print("="*80 + "\n")


def main():
    """ä¸»å‡½æ•°"""
    print("\n")
    print("â•”" + "â•"*78 + "â•—")
    print("â•‘" + " "*15 + "CogVideoX1.5 è§†é¢‘é®æŒ¡æ·»åŠ å·¥å…· - ä¿®å¤ç‰ˆ" + " "*23 + "â•‘")
    print("â•š" + "â•"*78 + "â•")
    
    processor = VideoOcclusionProcessor(CONFIG)
    processor.print_system_info()
    
    print("å½“å‰é…ç½®:")
    print(f"  ğŸ“‚ è¾“å…¥ç›®å½•: {CONFIG['video_input_dir']}")
    print(f"  ğŸ“‚ è¾“å‡ºç›®å½•: {CONFIG['output_dir']}")
    print(f"  ğŸ“‚ æ¨¡å‹ç›®å½•: {CONFIG['model_dir']}")
    print(f"  ğŸ¤– æ¨¡å‹: {CONFIG['model_id']}")
    print(f"  ğŸ² é®æŒ¡æ–¹å¼: {'éšæœº' if CONFIG['random_occlusion'] else 'é¡ºåº'}")
    print(f"  ğŸ¬ è¾“å‡ºè§„æ ¼: {CONFIG['num_frames']}å¸§ @ {CONFIG['fps']}fps")
    print(f"  ğŸ’¾ æ•°æ®ç±»å‹: {CONFIG['dtype']}")
    
    print("\n" + "="*80)
    response = input("ç¡®è®¤é…ç½®æ— è¯¯ï¼ŒæŒ‰Enteré”®å¼€å§‹å¤„ç† (æˆ–è¾“å…¥ q é€€å‡º): ")
    if response.lower() == 'q':
        print("å·²å–æ¶ˆ")
        return
    
    processor.setup_model()
    processor.process_all_videos()
    
    print("ğŸ‰ ç¨‹åºç»“æŸ!\n")


if __name__ == "__main__":
    main()