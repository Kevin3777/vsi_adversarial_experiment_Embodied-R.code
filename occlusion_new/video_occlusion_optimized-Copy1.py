"""
CogVideoX1.5 è§†é¢‘é®æŒ¡æ·»åŠ å·¥å…· - æ€§èƒ½ä¼˜åŒ–ç‰ˆ
ä½¿ç”¨æœ€æ–°çš„ CogVideoX1.5-5B-I2V æ¨¡å‹ï¼Œæ•ˆæœæ›´å¥½ï¼Œé€Ÿåº¦æ›´å¿«

ä½¿ç”¨æ–¹æ³•:
    python video_occlusion_optimized.py

ä½œè€…: AI Assistant
æ—¥æœŸ: 2025-10-05
"""

import os
import random
import torch
import cv2
from pathlib import Path
from diffusers import CogVideoXImageToVideoPipeline
from diffusers.utils import export_to_video
from PIL import Image
import warnings
import time
warnings.filterwarnings('ignore')

# ==================== é…ç½®åŒºåŸŸ ====================
CONFIG = {
    # è·¯å¾„é…ç½®
    'video_input_dir': "/root/autodl-tmp/sida/urban_videos/original",
    'model_dir': "/root/autodl-tmp/sida/occlusion_new/models",
    'output_dir': "/root/autodl-tmp/sida/urban_videos/with_occlusion",
    
    # æ¨¡å‹é…ç½® - ä½¿ç”¨æœ€æ–°çš„1.5ç‰ˆæœ¬
    'model_id': "THUDM/CogVideoX1.5-5B-I2V",
    
    # ç”Ÿæˆå‚æ•°ï¼ˆä¼˜åŒ–åçš„æ¨èå€¼ï¼‰
    'num_inference_steps': 50,      # æ¨ç†æ­¥æ•°
    'num_frames': 81,               # 1.5ç‰ˆæœ¬æ”¯æŒ81å¸§ï¼ˆæ›´é•¿ï¼‰
    'guidance_scale': 6.0,          # å¼•å¯¼ç³»æ•°
    'fps': 16,                      # 1.5ç‰ˆæœ¬æ¨è16fps
    'seed': 42,                     # éšæœºç§å­
    
    # å†…å­˜ä¼˜åŒ–ï¼ˆ32GBæ˜¾å­˜å»ºè®®é…ç½®ï¼‰
    'use_cpu_offload': False,       # 32GBæ˜¾å­˜å¯ä»¥ä¸ç”¨CPU offload
    'use_vae_slicing': True,        # å¼€å¯VAEåˆ‡ç‰‡
    'use_vae_tiling': True,         # å¼€å¯VAEå¹³é“º
    'dtype': torch.bfloat16,        # ä½¿ç”¨BF16è·å¾—æ›´å¥½æ•ˆæœ
    
    # å¤„ç†é€‰é¡¹
    'random_occlusion': True,       # éšæœºé€‰æ‹©é®æŒ¡
    'max_size': 1360,               # å›¾åƒæœ€å¤§è¾¹
    'min_size': 768,                # å›¾åƒæœ€å°è¾¹
    'batch_size': 1,                # æ‰¹å¤„ç†å¤§å°
}

# é®æŒ¡æç¤ºè¯ï¼ˆç»è¿‡ä¼˜åŒ–çš„promptsï¼‰
OCCLUSION_PROMPTS = [
    "A close-up car in the foreground partially blocking the view, with a vehicle silhouette passing by the camera, creating a natural street perspective with realistic automobile obstruction",
    "Tree branches and leaves in the foreground, natural foliage partially blocking the camera view, with a tree trunk visible in front, creating an outdoor perspective with vegetation obstruction",
    "A building corner in the foreground, architectural elements blocking part of the view, wall edge obscuring the perspective, urban structure creating a natural city obstruction",
    "A person walking directly in front of the camera, pedestrian silhouette in the foreground, human figure partially blocking the street view, realistic passerby obstruction",
    "Traffic sign pole in the foreground, street signage blocking part of the view, urban infrastructure obstruction with road signs clearly visible in front",
    "Various foreground objects partially blocking the view, natural urban elements obstructing the camera, street furniture creating realistic perspective with obstacles",
    "Fence or metal railing in the foreground, barrier partially blocking the street view, guardrail obstruction creating depth in the urban scene",
    "Bus stop shelter in the foreground, urban furniture blocking part of the view, street infrastructure with city elements clearly visible in front of camera"
]


class VideoOcclusionProcessor:
    """è§†é¢‘é®æŒ¡å¤„ç†å™¨ - ä¼˜åŒ–ç‰ˆ"""
    
    def __init__(self, config):
        self.config = config
        self.pipe = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.processing_stats = {
            'total_videos': 0,
            'successful': 0,
            'failed': 0,
            'total_time': 0
        }
        
    def print_system_info(self):
        """æ‰“å°ç³»ç»Ÿä¿¡æ¯"""
        print("\n" + "="*80)
        print("ç³»ç»Ÿä¿¡æ¯".center(80))
        print("="*80)
        
        if torch.cuda.is_available():
            print(f"âœ“ GPU: {torch.cuda.get_device_name(0)}")
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"âœ“ æ˜¾å­˜: {total_memory:.2f} GB")
            print(f"âœ“ CUDAç‰ˆæœ¬: {torch.version.cuda}")
        else:
            print("âœ— è­¦å‘Š: æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPU (éå¸¸æ…¢)")
            
        print(f"âœ“ PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"âœ“ è®¾å¤‡: {self.device}")
        print("="*80 + "\n")
        
    def setup_model(self):
        """åŠ è½½æ¨¡å‹ - å¢å¼ºæ–­ç‚¹ç»­ä¼ ç‰ˆ"""
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {self.config['model_id']}")
        print(f"æ¨¡å‹ä¿å­˜è·¯å¾„: {self.config['model_dir']}\n")
        
        os.makedirs(self.config['model_dir'], exist_ok=True)
        
        # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ä¼˜åŒ–ä¸‹è½½
        os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'  # ä½¿ç”¨é•œåƒï¼ˆå›½å†…ç”¨æˆ·ï¼‰
        os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'  # å¯ç”¨é«˜é€Ÿä¼ è¾“
        
        start_time = time.time()
        
        print("ğŸ“¥ ä¸‹è½½/åŠ è½½æ¨¡å‹ä¸­ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰...")
        
        # å¯¼å…¥å¿…è¦çš„åº“
        from huggingface_hub import snapshot_download
        import time as time_module
        
        # é‡è¯•é…ç½®
        max_retries = 5
        retry_delay = 10
        
        # å…ˆä½¿ç”¨ snapshot_download ä¸‹è½½æ¨¡å‹ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
        model_downloaded = False
        for attempt in range(max_retries):
            try:
                print(f"\nå°è¯•ä¸‹è½½æ¨¡å‹æ–‡ä»¶ (ç¬¬ {attempt + 1}/{max_retries} æ¬¡)...")
                snapshot_download(
                    repo_id=self.config['model_id'],
                    cache_dir=self.config['model_dir'],
                    resume_download=True,  # å…³é”®ï¼šå¯ç”¨æ–­ç‚¹ç»­ä¼ 
                    max_workers=4,  # å¹¶è¡Œä¸‹è½½
                    local_files_only=False,
                    ignore_patterns=["*.md", "*.txt"]  # è·³è¿‡ä¸å¿…è¦çš„æ–‡ä»¶
                )
                print("âœ“ æ¨¡å‹æ–‡ä»¶ä¸‹è½½å®Œæˆ")
                model_downloaded = True
                break
                
            except KeyboardInterrupt:
                print("\nâš ï¸  ä¸‹è½½è¢«ç”¨æˆ·ä¸­æ–­")
                print("ğŸ’¡ æç¤ºï¼šå†æ¬¡è¿è¡Œè„šæœ¬å°†ä»æ–­ç‚¹ç»§ç»­ä¸‹è½½")
                raise
                
            except Exception as e:
                if attempt < max_retries - 1:
                    print(f"âš ï¸  ä¸‹è½½é‡åˆ°é”™è¯¯: {str(e)[:100]}")
                    print(f"â³ ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                    time_module.sleep(retry_delay)
                    retry_delay = min(retry_delay * 2, 120)  # æŒ‡æ•°é€€é¿ï¼Œæœ€å¤š2åˆ†é’Ÿ
                else:
                    print("âŒ å¤šæ¬¡é‡è¯•åä»ç„¶å¤±è´¥")
                    print("ğŸ’¡ å°è¯•ä½¿ç”¨å·²ä¸‹è½½çš„éƒ¨åˆ†æ–‡ä»¶ç»§ç»­...")
        
        # åŠ è½½ pipeline
        try:
            print("\nğŸ“¦ æ­£åœ¨åŠ è½½æ¨¡å‹åˆ°å†…å­˜...")
            self.pipe = CogVideoXImageToVideoPipeline.from_pretrained(
                self.config['model_id'],
                torch_dtype=self.config['dtype'],
                cache_dir=self.config['model_dir'],
                local_files_only=model_downloaded,  # å¦‚æœä¸‹è½½å®Œæˆï¼Œåªä½¿ç”¨æœ¬åœ°æ–‡ä»¶
                resume_download=True
            )
            
        except Exception as e:
            print(f"\nâŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("\nå¯èƒ½çš„åŸå› :")
            print("1. æ¨¡å‹æ–‡ä»¶æœªå®Œå…¨ä¸‹è½½")
            print("2. æ˜¾å­˜ä¸è¶³")
            print("3. ä¾èµ–åº“ç‰ˆæœ¬ä¸å…¼å®¹")
            print("\nå»ºè®®æ“ä½œ:")
            print("- å†æ¬¡è¿è¡Œè„šæœ¬ç»§ç»­ä¸‹è½½")
            print("- æ£€æŸ¥ç£ç›˜ç©ºé—´æ˜¯å¦å……è¶³ï¼ˆéœ€è¦çº¦20GBï¼‰")
            print(f"- æ£€æŸ¥æ¨¡å‹ç›®å½•: {self.config['model_dir']}")
            raise
        
        # æ ¹æ®æ˜¾å­˜æƒ…å†µåº”ç”¨ä¼˜åŒ–
        if self.config['use_cpu_offload']:
            print("âœ“ å¯ç”¨CPU offloadä¼˜åŒ–")
            self.pipe.enable_model_cpu_offload()
        else:
            print("âœ“ æ¨¡å‹åŠ è½½åˆ°GPU")
            self.pipe.to(self.device)
            
        if self.config['use_vae_slicing']:
            print("âœ“ å¯ç”¨VAEåˆ‡ç‰‡ä¼˜åŒ–")
            self.pipe.vae.enable_slicing()
            
        if self.config['use_vae_tiling']:
            print("âœ“ å¯ç”¨VAEå¹³é“ºä¼˜åŒ–")
            self.pipe.vae.enable_tiling()
        
        load_time = time.time() - start_time
        print(f"\nâœ“ æ¨¡å‹åŠ è½½å®Œæˆ! æ€»è€—æ—¶: {load_time:.2f}ç§’\n")
        
    def extract_first_frame(self, video_path):
        """æå–è§†é¢‘ç¬¬ä¸€å¸§"""
        cap = cv2.VideoCapture(video_path)
        ret, frame = cap.read()
        
        if not ret:
            cap.release()
            raise ValueError(f"æ— æ³•è¯»å–è§†é¢‘: {video_path}")
        
        # è·å–è§†é¢‘ä¿¡æ¯
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        cap.release()
        
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        return Image.fromarray(frame_rgb), fps, total_frames
    
    def resize_image(self, image):
        """è°ƒæ•´å›¾åƒå°ºå¯¸ - ä¼˜åŒ–ç‰ˆ"""
        w, h = image.size
        
        # è®¡ç®—æ–°å°ºå¯¸
        min_dim = min(w, h)
        max_dim = max(w, h)
        
        if min_dim < self.config['min_size']:
            scale = self.config['min_size'] / min_dim
            new_w, new_h = int(w * scale), int(h * scale)
        elif max_dim > self.config['max_size']:
            scale = self.config['max_size'] / max_dim
            new_w, new_h = int(w * scale), int(h * scale)
        else:
            new_w, new_h = w, h
        
        # ç¡®ä¿å°ºå¯¸èƒ½è¢«16æ•´é™¤ï¼ˆé‡è¦ï¼ï¼‰
        new_w = (new_w // 16) * 16
        new_h = (new_h // 16) * 16
        
        # ä½¿ç”¨é«˜è´¨é‡é‡é‡‡æ ·
        return image.resize((new_w, new_h), Image.Resampling.LANCZOS)
    
    def generate_video(self, video_path, output_path, occlusion_prompt):
        """ç”Ÿæˆå¸¦é®æŒ¡çš„è§†é¢‘ - ä¼˜åŒ–ç‰ˆ"""
        print(f"\n{'='*80}")
        print(f"å¤„ç†è§†é¢‘: {os.path.basename(video_path)}")
        print(f"{'='*80}")
        
        start_time = time.time()
        
        try:
            # æå–ç¬¬ä¸€å¸§å’Œè§†é¢‘ä¿¡æ¯
            first_frame, original_fps, total_frames = self.extract_first_frame(video_path)
            original_size = first_frame.size
            
            # è°ƒæ•´å›¾åƒå°ºå¯¸
            first_frame = self.resize_image(first_frame)
            print(f"ğŸ“ å›¾åƒå°ºå¯¸: {original_size} -> {first_frame.size}")
            print(f"ğŸ¬ åŸå§‹è§†é¢‘: {original_fps}fps, {total_frames}å¸§")
            
            # æ„å»ºä¼˜åŒ–çš„prompt
            full_prompt = (
                f"A high-quality urban street scene with realistic motion and natural lighting. "
                f"{occlusion_prompt}. "
                f"The scene maintains consistent dynamics and smooth motion. "
                f"Cinematic quality, detailed foreground and background, natural perspective."
            )
            
            print(f"ğŸ“ é®æŒ¡æ•ˆæœ: {occlusion_prompt[:70]}...")
            print(f"\nâš™ï¸  ç”Ÿæˆå‚æ•°:")
            print(f"   - æ¨ç†æ­¥æ•°: {self.config['num_inference_steps']}")
            print(f"   - ç”Ÿæˆå¸§æ•°: {self.config['num_frames']}")
            print(f"   - è¾“å‡ºå¸§ç‡: {self.config['fps']}fps")
            print(f"   - å¼•å¯¼ç³»æ•°: {self.config['guidance_scale']}")
            
            # ç”Ÿæˆè§†é¢‘
            print(f"\nğŸ¨ æ­£åœ¨ç”Ÿæˆè§†é¢‘...")
            
            generator = torch.Generator(device=self.device)
            if self.config['seed'] is not None:
                generator.manual_seed(self.config['seed'])
            
            # æ¸…ç†æ˜¾å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            video_frames = self.pipe(
                prompt=full_prompt,
                image=first_frame,
                num_videos_per_prompt=1,
                num_inference_steps=self.config['num_inference_steps'],
                num_frames=self.config['num_frames'],
                guidance_scale=self.config['guidance_scale'],
                generator=generator,
            ).frames[0]
            
            # ä¿å­˜è§†é¢‘
            print(f"ğŸ’¾ ä¿å­˜è§†é¢‘...")
            export_to_video(video_frames, output_path, fps=self.config['fps'])
            
            generation_time = time.time() - start_time
            print(f"\nâœ“ å®Œæˆ! è€—æ—¶: {generation_time:.2f}ç§’")
            print(f"âœ“ ä¿å­˜è‡³: {output_path}\n")
            
            return True, generation_time
            
        except Exception as e:
            error_time = time.time() - start_time
            print(f"\nâœ— é”™è¯¯: {str(e)}")
            print(f"âœ— å¤±è´¥è€—æ—¶: {error_time:.2f}ç§’\n")
            return False, error_time
    
    def process_all_videos(self):
        """æ‰¹é‡å¤„ç†æ‰€æœ‰è§†é¢‘"""
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.config['output_dir'], exist_ok=True)
        
        # è·å–è§†é¢‘æ–‡ä»¶
        video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.MP4', '.AVI', '.MOV']
        video_files = []
        for ext in video_extensions:
            video_files.extend(Path(self.config['video_input_dir']).glob(f'*{ext}'))
        
        if not video_files:
            print(f"âŒ é”™è¯¯: åœ¨ {self.config['video_input_dir']} ä¸­æœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶!")
            return
        
        self.processing_stats['total_videos'] = len(video_files)
        print(f"\nğŸ“ æ‰¾åˆ° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶\n")
        
        # æ˜¾ç¤ºå°†è¦å¤„ç†çš„æ–‡ä»¶
        print("å¾…å¤„ç†è§†é¢‘:")
        for i, vf in enumerate(video_files, 1):
            print(f"  {i}. {vf.name}")
        print()
        
        # å¤„ç†æ¯ä¸ªè§†é¢‘
        for i, video_path in enumerate(video_files, 1):
            print(f"\n{'#'*80}")
            print(f"è¿›åº¦: [{i}/{len(video_files)}]")
            print(f"{'#'*80}")
            
            # é€‰æ‹©é®æŒ¡æ•ˆæœ
            if self.config['random_occlusion']:
                occlusion_prompt = random.choice(OCCLUSION_PROMPTS)
            else:
                occlusion_prompt = OCCLUSION_PROMPTS[i % len(OCCLUSION_PROMPTS)]
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            output_filename = f"{video_path.stem}_occluded.mp4"
            output_path = os.path.join(self.config['output_dir'], output_filename)
            
            # ç”Ÿæˆè§†é¢‘
            success, duration = self.generate_video(
                str(video_path),
                output_path,
                occlusion_prompt
            )
            
            if success:
                self.processing_stats['successful'] += 1
            else:
                self.processing_stats['failed'] += 1
            
            self.processing_stats['total_time'] += duration
            
            # æ¸…ç†æ˜¾å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        self.print_summary()
    
    def print_summary(self):
        """æ‰“å°å¤„ç†æ‘˜è¦"""
        print("\n" + "="*80)
        print("å¤„ç†å®Œæˆç»Ÿè®¡".center(80))
        print("="*80)
        print(f"æ€»è§†é¢‘æ•°: {self.processing_stats['total_videos']}")
        print(f"æˆåŠŸ: {self.processing_stats['successful']} âœ“")
        print(f"å¤±è´¥: {self.processing_stats['failed']} âœ—")
        print(f"æ€»è€—æ—¶: {self.processing_stats['total_time']:.2f}ç§’")
        
        if self.processing_stats['successful'] > 0:
            avg_time = self.processing_stats['total_time'] / self.processing_stats['successful']
            print(f"å¹³å‡æ¯ä¸ªè§†é¢‘: {avg_time:.2f}ç§’")
        
        print(f"\nè¾“å‡ºç›®å½•: {self.config['output_dir']}")
        print("="*80 + "\n")


def main():
    """ä¸»å‡½æ•°"""
    print("\n")
    print("â•”" + "â•"*78 + "â•—")
    print("â•‘" + " "*15 + "CogVideoX1.5 è§†é¢‘é®æŒ¡æ·»åŠ å·¥å…· - ä¼˜åŒ–ç‰ˆ" + " "*23 + "â•‘")
    print("â•š" + "â•"*78 + "â•")
    
    # åˆå§‹åŒ–å¤„ç†å™¨
    processor = VideoOcclusionProcessor(CONFIG)
    
    # æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
    processor.print_system_info()
    
    # æ˜¾ç¤ºé…ç½®
    print("å½“å‰é…ç½®:")
    print(f"  ğŸ“‚ è¾“å…¥ç›®å½•: {CONFIG['video_input_dir']}")
    print(f"  ğŸ“‚ è¾“å‡ºç›®å½•: {CONFIG['output_dir']}")
    print(f"  ğŸ“‚ æ¨¡å‹ç›®å½•: {CONFIG['model_dir']}")
    print(f"  ğŸ¤– æ¨¡å‹: {CONFIG['model_id']}")
    print(f"  ğŸ² é®æŒ¡æ–¹å¼: {'éšæœº' if CONFIG['random_occlusion'] else 'é¡ºåº'}")
    print(f"  ğŸ¬ è¾“å‡ºè§„æ ¼: {CONFIG['num_frames']}å¸§ @ {CONFIG['fps']}fps")
    print(f"  ğŸ’¾ æ•°æ®ç±»å‹: {CONFIG['dtype']}")
    
    # ç¡®è®¤å¼€å§‹
    print("\n" + "="*80)
    response = input("ç¡®è®¤é…ç½®æ— è¯¯ï¼ŒæŒ‰Enteré”®å¼€å§‹å¤„ç† (æˆ–è¾“å…¥ q é€€å‡º): ")
    if response.lower() == 'q':
        print("å·²å–æ¶ˆ")
        return
    
    # åŠ è½½æ¨¡å‹
    processor.setup_model()
    
    # å¤„ç†è§†é¢‘
    processor.process_all_videos()
    
    print("ğŸ‰ ç¨‹åºç»“æŸ!\n")


if __name__ == "__main__":
    main()